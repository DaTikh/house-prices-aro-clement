{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-datasets\" data-toc-modified-id=\"The-datasets-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-full\" data-toc-modified-id=\"Train-full-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Train full</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-full-report-:-import-from-pandas_profiling\" data-toc-modified-id=\"Train-full-report-:-import-from-pandas_profiling-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Train full report : import from pandas_profiling</a></span></li></ul></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Test</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-report-:-import-from-pandas_profiling\" data-toc-modified-id=\"Test-report-:-import-from-pandas_profiling-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Test report : import from pandas_profiling</a></span></li></ul></li></ul></li><li><span><a href=\"#Dataset-adjusted-:-v2\" data-toc-modified-id=\"Dataset-adjusted-:-v2-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset adjusted : v2</a></span><ul class=\"toc-item\"><li><span><a href=\"#visualize-Train-v2\" data-toc-modified-id=\"visualize-Train-v2-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>visualize Train v2</a></span></li><li><span><a href=\"#Split-Train-v2\" data-toc-modified-id=\"Split-Train-v2-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Split Train v2</a></span></li><li><span><a href=\"#Test-v2:-Match-dataset-Test-to-train-v2\" data-toc-modified-id=\"Test-v2:-Match-dataset-Test-to-train-v2-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Test v2: Match dataset Test to train v2</a></span></li><li><span><a href=\"#Checking-data-for-learning\" data-toc-modified-id=\"Checking-data-for-learning-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Checking data for learning</a></span></li></ul></li><li><span><a href=\"#XBOOST-Normal\" data-toc-modified-id=\"XBOOST-Normal-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>XBOOST Normal</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning\" data-toc-modified-id=\"Learning-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Learning</a></span></li><li><span><a href=\"#Submission-to-Kaggle\" data-toc-modified-id=\"Submission-to-Kaggle-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Submission to Kaggle</a></span></li></ul></li><li><span><a href=\"#XGBoost-Optimized\" data-toc-modified-id=\"XGBoost-Optimized-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>XGBoost Optimized</a></span><ul class=\"toc-item\"><li><span><a href=\"#learning\" data-toc-modified-id=\"learning-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>learning</a></span></li><li><span><a href=\"#Submission-to-Kaggle\" data-toc-modified-id=\"Submission-to-Kaggle-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Submission to Kaggle</a></span></li></ul></li><li><span><a href=\"#XGBOOST-PCA\" data-toc-modified-id=\"XGBOOST-PCA-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>XGBOOST PCA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning\" data-toc-modified-id=\"Learning-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Learning</a></span></li><li><span><a href=\"#Submission-on-Kaggle\" data-toc-modified-id=\"Submission-on-Kaggle-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Submission on Kaggle</a></span></li></ul></li><li><span><a href=\"#KERAS\" data-toc-modified-id=\"KERAS-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>KERAS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning\" data-toc-modified-id=\"Learning-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Learning</a></span></li><li><span><a href=\"#Test-NN-with-no-hidden-layers\" data-toc-modified-id=\"Test-NN-with-no-hidden-layers-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Test NN with no hidden layers</a></span></li><li><span><a href=\"#Test-NN-with-several-Hidden-Layers\" data-toc-modified-id=\"Test-NN-with-several-Hidden-Layers-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Test NN with several Hidden Layers</a></span></li><li><span><a href=\"#Submission-to-Kaggle\" data-toc-modified-id=\"Submission-to-Kaggle-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Submission to Kaggle</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:33.987113Z",
     "start_time": "2019-12-09T23:42:33.534071Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.044136Z",
     "start_time": "2019-12-09T23:42:33.989540Z"
    }
   },
   "outputs": [],
   "source": [
    "data = read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.071741Z",
     "start_time": "2019-12-09T23:42:34.046843Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full = data\n",
    "df_train_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train full report : import from pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.076216Z",
     "start_time": "2019-12-09T23:42:34.073932Z"
    }
   },
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "# To diplay report in a html file\n",
    "# To run once and then read the output file when you want\n",
    "#ReportFullTrain = df_train_full.profile_report(title='Train full Profiling Report')\n",
    "#ReportFullTrain.to_file(output_file=\"HP-train-full-Report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.158832Z",
     "start_time": "2019-12-09T23:42:34.079438Z"
    }
   },
   "outputs": [],
   "source": [
    "data = read_csv('data/test.csv')\n",
    "df_test = data.copy()\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test report : import from pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.163367Z",
     "start_time": "2019-12-09T23:42:34.160911Z"
    }
   },
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "# To diplay report in a html file\n",
    "# To run once and then read the output file when you want\n",
    "#ReportTest = df_test.profile_report(title='Test Profiling Report')\n",
    "#ReportTest.to_file(output_file=\"HP-Test-Report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset adjusted : v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.213440Z",
     "start_time": "2019-12-09T23:42:34.165829Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full_num = df_train_full.select_dtypes(exclude=['object'])\n",
    "df_train_full_cat = df_train_full.select_dtypes(include=['object'])\n",
    "print(df_train_full.shape)\n",
    "print(df_train_full_num.shape)\n",
    "print(df_train_full_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.229527Z",
     "start_time": "2019-12-09T23:42:34.222590Z"
    }
   },
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#sns.scatterplot(x=train_full['Surface'],\n",
    "#                y=train_full['SalePrice'], hue=train_full['Neighborhood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.325468Z",
     "start_time": "2019-12-09T23:42:34.233103Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full_dum_v2 = pd.get_dummies(df_train_full_cat)\n",
    "df_train_full_dum_v2 = df_train_full_dum_v2.join(df_train_full['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.336601Z",
     "start_time": "2019-12-09T23:42:34.327492Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full_dum_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.371934Z",
     "start_time": "2019-12-09T23:42:34.338461Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full_dum_v2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.400277Z",
     "start_time": "2019-12-09T23:42:34.374003Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_train_full_num_v2 = df_train_full_num.dropna(axis=0)\n",
    "df_train_full_num_v2 = df_train_full_num.fillna(0)\n",
    "df_train_full_num_v2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.407295Z",
     "start_time": "2019-12-09T23:42:34.402636Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_train_full_num.shape)\n",
    "print(df_train_full_num_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.429598Z",
     "start_time": "2019-12-09T23:42:34.409353Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full_v2 = df_train_full_num_v2.join(df_train_full_dum_v2.set_index('Id'), on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.465605Z",
     "start_time": "2019-12-09T23:42:34.431751Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full_v2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.472544Z",
     "start_time": "2019-12-09T23:42:34.468793Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.519935Z",
     "start_time": "2019-12-09T23:42:34.474485Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_full_v2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize Train v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:34.523431Z",
     "start_time": "2019-12-09T23:42:34.521648Z"
    }
   },
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "#df_train_full_v2.profile_report(style={'full_width':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:35.099573Z",
     "start_time": "2019-12-09T23:42:34.525409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data\n",
    "df_train_for_learning = df_train_full_v2.copy()\n",
    "df_train_for_learning = df_train_for_learning.drop('Id', axis=1)\n",
    "    \n",
    "#Split\n",
    "val_size = 0.2\n",
    "X_train_v2, X_val_v2 = train_test_split(df_train_for_learning, test_size = val_size)\n",
    "#print(\"X Train full shape: \" + str(df_train_for_model.shape))\n",
    "\n",
    "#Separated x and y\n",
    "target_column = 'SalePrice'\n",
    "y_train_v2 = X_train_v2[target_column]\n",
    "X_train_v2 = X_train_v2.drop(target_column, axis=1)\n",
    "y_train_v2 = pd.DataFrame(y_train_v2)    \n",
    "X_train_v2 = pd.DataFrame(X_train_v2)\n",
    "\n",
    "\n",
    "y_val_v2 = X_val_v2[target_column]\n",
    "X_val_v2 = X_val_v2.drop(target_column, axis=1)\n",
    "y_val_v2 = pd.DataFrame(y_val_v2)    \n",
    "X_val_v2 = pd.DataFrame(X_val_v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test v2: Match dataset Test to train v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:35.202730Z",
     "start_time": "2019-12-09T23:42:35.101830Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_num = df_test.select_dtypes(exclude=['object'])\n",
    "df_test_cat = df_test.select_dtypes(include=['object'])\n",
    "#\n",
    "df_test_dum_v2 = pd.get_dummies(df_test_cat)\n",
    "df_test_dum_v2 = df_test_dum_v2.join(df_test['Id'])\n",
    "#\n",
    "df_test_num_v2 = df_test_num.fillna(0)\n",
    "df_test_num_v2.head(5)\n",
    "#\n",
    "df_test_v2 = df_test_num_v2.join(df_test_dum_v2.set_index('Id'), on='Id')\n",
    "\n",
    "print(df_test.shape)\n",
    "#\n",
    "print(df_test_num.shape)\n",
    "print(df_test_cat.shape)\n",
    "#\n",
    "print(df_test_num_v2.shape)\n",
    "print(df_test_dum_v2.shape)\n",
    "#\n",
    "print(df_test_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:35.352277Z",
     "start_time": "2019-12-09T23:42:35.204630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Column test to adjust\n",
    "missing_col = list(set(df_train_full_v2.columns) - set(df_test.columns))\n",
    "for col in missing_col:\n",
    "    df_test[col] = 0\n",
    "df_test_v2 = df_test[df_train_full_v2.columns]\n",
    "df_test_v2 = df_test_v2.fillna(0) \n",
    "df_test_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:35.382010Z",
     "start_time": "2019-12-09T23:42:35.354388Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_v2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:35.396485Z",
     "start_time": "2019-12-09T23:42:35.386290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Full\n",
    "print('**Train full**')\n",
    "df_train_full_v2\n",
    "print('df_train_full_v2', df_train_full_v2.shape)\n",
    "print()\n",
    "\n",
    "# Train splitted\n",
    "print('**Train splitted**')\n",
    "X_train_v2\n",
    "y_train_v2\n",
    "print('X_train_v2', X_train_v2.shape)\n",
    "print('y_train_v2', y_train_v2.shape)\n",
    "print()\n",
    "X_val_v2\n",
    "y_val_v2\n",
    "print('X_val_v2', X_val_v2.shape)\n",
    "print('y_val_v2', y_val_v2.shape)\n",
    "print()\n",
    "\n",
    "# Test\n",
    "print('**Test**')\n",
    "df_test_v2\n",
    "print('df_test_v2',df_test_v2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XBOOST Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.701708Z",
     "start_time": "2019-12-09T15:35:31.541310Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X_train = X_train.copy()\n",
    "#y_train = np.log (y_train)\n",
    "\n",
    "xgbReg_model = xgb.XGBRegressor()\n",
    "xgbReg_model.fit(X_train,y_train) \n",
    "\n",
    "y_predict = xgbReg_model.predict(X_train)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print()\n",
    "print('XGBOOST')\n",
    "print ('Results on train')\n",
    "print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_train, y_predict)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_train, y_predict))\n",
    "print()\n",
    "\n",
    "\n",
    "print ('Results on val')\n",
    "y_predict_val = xgbReg_model.predict(X_val)\n",
    "print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_val, y_predict_val)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_val, y_predict_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.706009Z",
     "start_time": "2019-12-09T15:35:29.549Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train_full_v2.copy()\n",
    "y_train = X_train['SalePrice'].copy()\n",
    "X_train = X_train.drop('Id', axis=1)\n",
    "X_train = X_train.drop('SalePrice', axis=1)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "X_test_id = df_test_v2['Id'].copy()\n",
    "X_test = df_test_v2.drop('Id', axis=1)\n",
    "X_test = X_test.drop('SalePrice', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.707484Z",
     "start_time": "2019-12-09T15:35:29.553Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train = pd.DataFrame(y_train)    \n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "\n",
    "xgbReg_model = xgb.XGBRegressor()\n",
    "xgbReg_model.fit(X_train,y_train) \n",
    "\n",
    "y_predict = xgbReg_model.predict(X_train)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print()\n",
    "print()\n",
    "print('XGBOOST')\n",
    "print ('Results on full')\n",
    "print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_train, y_predict)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_train, y_predict))\n",
    "print()\n",
    "\n",
    "y_predict_test = xgbReg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.709185Z",
     "start_time": "2019-12-09T15:35:29.557Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_id = pd.DataFrame(X_test_id)\n",
    "y_predict_test = pd.DataFrame(y_predict_test)\n",
    "df_sub = X_test_id.join(y_predict_test)\n",
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.711700Z",
     "start_time": "2019-12-09T15:35:29.562Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names_subm = df_sub.columns.values\n",
    "col_names_subm[1] = 'SalePrice'\n",
    "df_sub.columns = col_names_subm\n",
    "df_sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.713372Z",
     "start_time": "2019-12-09T15:35:29.565Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv('hp_all_xgb3.csv', index=False)\n",
    "import kaggle\n",
    "submission_file = \"hp_all_xgb3.csv\"\n",
    "kaggle.api.competition_submit(submission_file, \"all_v0_xgb3\", \"house-prices-advanced-regression-techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.714637Z",
     "start_time": "2019-12-09T15:35:29.570Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.copy()\n",
    "y_train = y_train.copy()\n",
    "X_val = X_val.copy()\n",
    "y_val = y_val.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.716019Z",
     "start_time": "2019-12-09T15:35:29.574Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#xgbReg_model = xgb.XGBRegressor(base_score=0.5, \n",
    "#                                booster='gbtree', \n",
    "#                                colsample_bylevel=1,\n",
    "#                                colsample_bynode=1, \n",
    "#                                colsample_bytree=1, \n",
    "#                                gamma=0,\n",
    "#                                importance_type='gain', \n",
    "#                                learning_rate=0.1, \n",
    "#                                max_delta_step=0,\n",
    "#                                max_depth=3, \n",
    "#                                min_child_weight=1, \n",
    "#                                missing=None, \n",
    "#                                n_estimators=100,\n",
    "#                                n_jobs=1, \n",
    "#                                nthread=None, \n",
    "#                                objective='reg:linear', \n",
    "#                                random_state=0,\n",
    "#                                reg_alpha=0, \n",
    "#                                reg_lambda=1, \n",
    "#                                scale_pos_weight=1, \n",
    "#                                seed=None,\n",
    "#                                silent=None, \n",
    "#                                subsample=1, \n",
    "#                                verbosity=1))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'n_estimators': [200,500,800,1000],\n",
    "               'max_depth': [5,8,10],\n",
    "               'learning_rate':[0.1,0.3,0.6],\n",
    "              }]\n",
    "my_learn = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(my_learn,param_grid, cv=5, scoring ='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.717361Z",
     "start_time": "2019-12-09T15:35:29.578Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.719452Z",
     "start_time": "2019-12-09T15:35:29.584Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print()\n",
    "print('XGBOOST')\n",
    "print ('Results on train')\n",
    "y_predict = grid_search.predict(X_train)\n",
    "print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_train, y_predict)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_train, y_predict))\n",
    "print()\n",
    "\n",
    "\n",
    "print ('Results on val')\n",
    "y_predict_val = grid_search.predict(X_val)\n",
    "print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_val, y_predict_val)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_val, y_predict_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.721438Z",
     "start_time": "2019-12-09T15:35:29.588Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train_full_v2.copy()\n",
    "y_train = X_train['SalePrice'].copy()\n",
    "X_train = X_train.drop('Id', axis=1)\n",
    "X_train = X_train.drop('SalePrice', axis=1)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "X_test_id = df_test_v2['Id'].copy()\n",
    "X_test = df_test_v2.drop('Id', axis=1)\n",
    "X_test = X_test.drop('SalePrice', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print (X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.723037Z",
     "start_time": "2019-12-09T15:35:29.592Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'n_estimators': [800],\n",
    "               'max_depth': [5],\n",
    "               'learning_rate':[0.1],\n",
    "              }]\n",
    "my_learn = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(my_learn,param_grid, cv=5, scoring ='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_predict = grid_search.predict(X_train)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print()\n",
    "print('XGBOOST')\n",
    "print ('Results on full')\n",
    "print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_train, y_predict)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_train, y_predict))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.724412Z",
     "start_time": "2019-12-09T15:35:29.595Z"
    }
   },
   "outputs": [],
   "source": [
    "y_predict_test = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.725966Z",
     "start_time": "2019-12-09T15:35:29.598Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_id = pd.DataFrame(X_test_id)\n",
    "y_predict_test = pd.DataFrame(y_predict_test)\n",
    "df_sub = X_test_id.join(y_predict_test)\n",
    "print(df_sub.shape)\n",
    "col_names_subm = df_sub.columns.values\n",
    "col_names_subm[1] = 'SalePrice'\n",
    "df_sub.columns = col_names_subm\n",
    "df_sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T15:35:31.727637Z",
     "start_time": "2019-12-09T15:35:29.604Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv('hp_all_xgb_opti.csv', index=False)\n",
    "import kaggle\n",
    "submission_file = \"hp_all_xgb_opti.csv\"\n",
    "kaggle.api.competition_submit(submission_file, \"all_v0_xgb_Opti\", \"house-prices-advanced-regression-techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:54:48.592991Z",
     "start_time": "2019-12-04T07:54:48.584594Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pca = X_train.copy()\n",
    "y_train_pca = y_train.copy()\n",
    "X_val_pca = X_val.copy()\n",
    "y_val_pca = y_val.copy()\n",
    "\n",
    "print(X_train_pca.shape)\n",
    "print(y_train_pca.shape)\n",
    "print(X_val_pca.shape)\n",
    "print(y_val_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:59:40.538982Z",
     "start_time": "2019-12-04T07:59:40.456159Z"
    }
   },
   "outputs": [],
   "source": [
    "#Instantiate model -full\n",
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA()\n",
    "inputs = X_train_pca.iloc[:,1:]\n",
    "pca_model.fit(inputs)\n",
    "\n",
    "cumsum = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "nb_dim95 = np.argmax(cumsum >= 0.95) + 1\n",
    "print(\"Dimension number\", df_train_full_v2.shape[1]-1)\n",
    "print(\"PC1, contribution of variance :\", pca_model.explained_variance_ratio_[0])\n",
    "print(\"PC2, contribution of variance :\", pca_model.explained_variance_ratio_[1])\n",
    "print(\"PC3, contribution of variance :\", pca_model.explained_variance_ratio_[2])\n",
    "print(\"Dimension number, explaining 95% of variance :\", nb_dim95)\n",
    "nb_dim95 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:59:46.304378Z",
     "start_time": "2019-12-04T07:59:43.173146Z"
    }
   },
   "outputs": [],
   "source": [
    "contrib = np.round(pca_model.explained_variance_ratio_*100, decimals = 1)\n",
    "labels = ['PC'+str(x) for x in range (1,len(contrib)+1)]\n",
    "plt.bar(x=range(1,len(contrib)+1), height = contrib, tick_label = labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:59:52.401879Z",
     "start_time": "2019-12-04T07:59:52.354319Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_dims = PCA(n_components=nb_dim95)\n",
    "X_train_pca_v2 = pca_dims.fit_transform(X_train_pca)\n",
    "X_val_pca_v2 = pca_dims.fit_transform(X_val_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:59:56.737415Z",
     "start_time": "2019-12-04T07:59:56.647393Z"
    }
   },
   "outputs": [],
   "source": [
    "#model\n",
    "model_pca = xgb.XGBRegressor()\n",
    "model_pca.fit(X_train_pca_v2,y_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T08:00:03.551647Z",
     "start_time": "2019-12-04T08:00:03.526310Z"
    }
   },
   "outputs": [],
   "source": [
    "#Evaluation on train\n",
    "y_train_pca_pred = model_pca.predict(X_train_pca_v2)\n",
    "y_val_pca_pred = model_pca.predict(X_val_pca_v2)\n",
    "\n",
    "print ('Results on train')\n",
    "print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_train, y_train_pca_pred)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_train, y_train_pca_pred))\n",
    "print()\n",
    "\n",
    "print ('Results on val')\n",
    "print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_val, y_val_pca_pred)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_val, y_val_pca_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = df_train_full_v2.copy()\n",
    "pca_dims = PCA(n_components=nb_dim95)\n",
    "X_train_pca_v2 = pca_dims.fit_transform(X_train_pca)\n",
    "X_val_pca_v2 = pca_dims.fit_transform(X_val_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more on [Keras](https://keras.io/models/sequential/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T20:51:01.055109Z",
     "start_time": "2019-12-06T20:51:01.048998Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train_v2.copy()\n",
    "y_train = y_train_v2.copy()\n",
    "X_val = X_val_v2.copy()\n",
    "y_val = y_val_v2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T20:51:09.224024Z",
     "start_time": "2019-12-06T20:51:05.993187Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                    Dense(1)\n",
    "                    ])\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "loss ='mse'\n",
    "LEARNING_RATE = 0.01\n",
    "model.compile(loss=loss, optimizer=SGD(lr=LEARNING_RATE))\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "scaler.fit(y_train)\n",
    "y_train = scaler.transform(np.log(y_train))\n",
    "\n",
    "#\n",
    "BATCH_SIZE = X_train.shape[0] # computing the loss over the whole dataset\n",
    "EPOCHS = 200 # how many iterations over the whole dataset\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T20:51:17.080394Z",
     "start_time": "2019-12-06T20:51:17.059576Z"
    }
   },
   "outputs": [],
   "source": [
    "# look what scaler perform\n",
    "#y_train_v2.info()\n",
    "y_train_log = np.log(y_train_v2)\n",
    "print(y_train_log.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y_train_log)\n",
    "X_train_log_transf = scaler.transform(y_train_log)\n",
    "#X_train_log_transf = scaler.fit_transform(y_train_log)\n",
    "pd.DataFrame(X_train_log_transf).info()\n",
    "invTransfYtrain = scaler.inverse_transform (X_train_log_transf)\n",
    "\n",
    "pd.DataFrame(invTransfYtrain).equals (y_train_log)\n",
    "\n",
    "\n",
    "\n",
    "#df_X_train = pd.DataFrame(X_train)\n",
    "#df_X_train.describe()\n",
    "#invTransXtrain = scaler.inverse_transform (df_X_train)\n",
    "#invTransXtrain = pd.DataFrame(invTransXtrain)\n",
    "#invTransXtrain.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T20:51:33.204632Z",
     "start_time": "2019-12-06T20:51:33.192426Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_log.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T08:09:51.889335Z",
     "start_time": "2019-12-06T08:09:51.877213Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(invTransfYtrain).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T20:51:40.254438Z",
     "start_time": "2019-12-06T20:51:39.997127Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "#plt.gca().set_ylim(0, 1)\n",
    "plt.title('Model performance throughout training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "print()\n",
    "print('KERAS')\n",
    "print ('Results on train')\n",
    "y_predict = model.predict(X_train)\n",
    "#print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_train, y_predict)))\n",
    "print('MAE : ', metrics.mean_absolute_error(y_train, y_predict))\n",
    "print()\n",
    "\n",
    "\n",
    "#print ('Results on val')\n",
    "#y_predict_val = model.predict(X_val)\n",
    "#print('RMSLE : ', np.sqrt(metrics.mean_squared_log_error(y_val, y_predict_val)))\n",
    "#print('MAE : ', metrics.mean_absolute_error(y_val, y_predict_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NN with no hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:42:46.145356Z",
     "start_time": "2019-12-09T23:42:46.123548Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train_full_v2.copy()\n",
    "y_train = X_train['SalePrice'].copy()\n",
    "X_train = X_train.drop('Id', axis=1)\n",
    "X_train = X_train.drop('SalePrice', axis=1)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "X_test_id = df_test_v2['Id'].copy()\n",
    "X_test = df_test_v2.drop('Id', axis=1)\n",
    "X_test = X_test.drop('SalePrice', axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print (X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T06:56:30.730300Z",
     "start_time": "2019-12-10T06:56:26.153643Z"
    },
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "#to generate the same number every time you need to pass the same seed value every time\n",
    "#Make results reproducible -> fix seed of TensorFlow's random number generator\n",
    "from numpy import random as np_random\n",
    "SEED = 42\n",
    "np_random.seed(SEED)\n",
    "from tensorflow import random as tf_random\n",
    "tf_random.set_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_transf = scaler.fit_transform(X_train)\n",
    "y_train_log = np.log(y_train)\n",
    "y_train_log_transf = scaler.fit_transform(y_train_log)\n",
    "\n",
    "\n",
    "\n",
    "#initial pgm which is working\n",
    "model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                   Dense(1)\n",
    "                 ])\n",
    "model.add(Dense(1))\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "#loss ='mean_squared_logarithmic_error'\n",
    "loss ='mse'\n",
    "LEARNING_RATE = 0.01\n",
    "model.compile(loss=loss, optimizer=SGD(lr=LEARNING_RATE))\n",
    "BATCH_SIZE = X_train.shape[0] # computing the loss over the whole dataset\n",
    "EPOCHS = 200 # how many iterations over the whole dataset\n",
    "history = model.fit(X_train_transf, \n",
    "                    y_train_log_transf, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=EPOCHS, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    verbose=0)\n",
    "\n",
    "\n",
    "#1/create NN Model\n",
    "#model = Sequential()\n",
    "#2/add layers \n",
    "#model.add(Dense(1,input_dim = X_train.shape[1]))\n",
    "#3/Compile \n",
    "#from tensorflow.keras.optimizers import SGD\n",
    "#model.compile(loss='mse', optimizer=SGD(lr=0.01))\n",
    "#4/fit\n",
    "#history = model.fit(X_train_transf,\n",
    "#                    y_train_log_transf, \n",
    "#                    validation_split=0.2, \n",
    "#                    epochs=200, \n",
    "#                    batch_size=X_train.shape[0],\n",
    "#                    verbose=0)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.title('Model performance throughout training')\n",
    "plt.ylim(0,0.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "print('loss')\n",
    "meanValLoss = np.mean(np.sqrt(history.history['val_loss']))\n",
    "minValLoss = np.min(np.sqrt(history.history['val_loss']))\n",
    "print('Mean on Val loss',meanValLoss )\n",
    "print('Min on Val loss',minValLoss )\n",
    "print()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "y_train_predict_log_transf = model.predict(X_train_transf)\n",
    "scaler = StandardScaler()\n",
    "y_train_log_transf_xxx = scaler.fit_transform(np.log(y_train))\n",
    "y_train_predict_log  = scaler.inverse_transform(y_train_predict_log_transf)\n",
    "y_train_predict  = np.exp(y_train_predict_log)\n",
    "\n",
    "print('RMSLE on Train set: ', np.sqrt(metrics.mean_squared_log_error(y_train, y_train_predict)))\n",
    "print('MAE on Train set: ', metrics.mean_absolute_error(y_train, y_train_predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:58:18.545127Z",
     "start_time": "2019-12-09T23:58:16.251021Z"
    },
    "cell_style": "split",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "#to generate the same number every time you need to pass the same seed value every time\n",
    "#Make results reproducible -> fix seed of TensorFlow's random number generator\n",
    "#from numpy import random as np_random\n",
    "#SEED = 42\n",
    "#np_random.seed(SEED)\n",
    "#from tensorflow import random as tf_random\n",
    "#tf_random.set_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_transf = scaler.fit_transform(X_train)\n",
    "y_train_log = np.log(y_train)\n",
    "y_train_log_transf = scaler.fit_transform(y_train_log)\n",
    "\n",
    "\n",
    "\n",
    "#initial pgm which is working\n",
    "model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                   Dense(1)\n",
    "                 ])\n",
    "model.add(Dense(1))\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "loss ='mean_squared_logarithmic_error'\n",
    "LEARNING_RATE = 0.01\n",
    "model.compile(loss=loss, optimizer=SGD(lr=LEARNING_RATE))\n",
    "BATCH_SIZE = X_train.shape[0] # computing the loss over the whole dataset\n",
    "EPOCHS = 200 # how many iterations over the whole dataset\n",
    "history = model.fit(X_train_transf, \n",
    "                    y_train_log_transf,\n",
    "                    validation_split=0.2, \n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose=0)\n",
    "\n",
    "\n",
    "#1/create NN Model\n",
    "#model = Sequential()\n",
    "#2/add layers \n",
    "#model.add(Dense(1,input_dim = X_train.shape[1]))\n",
    "#3/Compile \n",
    "#from tensorflow.keras.optimizers import SGD\n",
    "#model.compile(loss='mean_squared_logarithmic_error', optimizer=SGD(lr=0.01))\n",
    "#4/fit\n",
    "#history = model.fit(X_train_transf, \n",
    "#                    y_train_log_transf, \n",
    "#                   validation_split=0.2, \n",
    "#                   epochs=500, \n",
    "#                    batch_size=X_train.shape[0],\n",
    "#                    verbose=0)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.title('Model performance throughout training')\n",
    "plt.ylim(0,0.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "print('loss')\n",
    "meanValLoss = np.mean(np.sqrt(history.history['val_loss']))\n",
    "minValLoss = np.min(np.sqrt(history.history['val_loss']))\n",
    "print('Mean on Val loss',meanValLoss )\n",
    "print('Min on Val loss',minValLoss )\n",
    "print()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "y_train_predict_log_transf = model.predict(X_train_transf)\n",
    "scaler = StandardScaler()\n",
    "y_train_log_transf_xxx = scaler.fit_transform(np.log(y_train))\n",
    "y_train_predict_log  = scaler.inverse_transform(y_train_predict_log_transf)\n",
    "y_train_predict  = np.exp(y_train_predict_log)\n",
    "\n",
    "print('RMSLE on Train set: ', np.sqrt(metrics.mean_squared_log_error(y_train, y_train_predict)))\n",
    "print('MAE on Train set: ', metrics.mean_absolute_error(y_train, y_train_predict))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## Test NN with several Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T07:01:52.541482Z",
     "start_time": "2019-12-10T07:01:44.427288Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "#to generate the same number every time you need to pass the same seed value every time\n",
    "#Make results reproducible -> fix seed of TensorFlow's random number generator\n",
    "from numpy import random as np_random\n",
    "SEED = 42\n",
    "np_random.seed(SEED)\n",
    "from tensorflow import random as tf_random\n",
    "tf_random.set_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_transf = scaler.fit_transform(X_train)\n",
    "y_train_log = np.log(y_train)\n",
    "y_train_log_transf = scaler.fit_transform(y_train_log)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_transf = scaler.fit_transform(X_train)\n",
    "y_train_log = np.log(y_train)\n",
    "y_train_log_transf = scaler.fit_transform(y_train_log)\n",
    "\n",
    "#initial pgm which is working\n",
    "n_input = X_train.shape[1]\n",
    "model = Sequential([\n",
    "    Input(shape=n_input),\n",
    "    Dense(n_input, activation='relu'),\n",
    "    Dense(n_input, activation='relu'),\n",
    "    Dense(n_input, activation='relu'),\n",
    "    Dense(n_input, activation='relu'),\n",
    " #  Dense(n_input, activation='relu'),  \n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "loss ='mean_squared_logarithmic_error'\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "model.compile(loss=loss, optimizer=SGD(lr=LEARNING_RATE))\n",
    "BATCH_SIZE = X_train.shape[0] # computing the loss over the whole dataset\n",
    "EPOCHS = 200 # how many iterations over the whole dataset\n",
    "history = model.fit(X_train_transf, \n",
    "                    y_train_log_transf, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=EPOCHS, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "#plt.gca().set_ylim(0, 1)\n",
    "plt.title('Model performance throughout training')\n",
    "plt.ylim(0,0.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "meanValLoss = np.mean(np.sqrt(history.history['val_loss']))\n",
    "minValLoss = np.min(np.sqrt(history.history['val_loss']))\n",
    "print('Mean on Val loss',meanValLoss )\n",
    "print('Min on Val loss',minValLoss )\n",
    "print()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "y_train_predict_log_transf = model.predict(X_train_transf)\n",
    "scaler = StandardScaler()\n",
    "y_train_log_transf_xxx = scaler.fit_transform(np.log(y_train))\n",
    "y_train_predict_log  = scaler.inverse_transform(y_train_predict_log_transf)\n",
    "y_train_predict  = np.exp(y_train_predict_log)\n",
    "\n",
    "print('RMSLE on Train set: ', np.sqrt(metrics.mean_squared_log_error(y_train, y_train_predict)))\n",
    "print('MAE on Train set: ', metrics.mean_absolute_error(y_train, y_train_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T19:21:58.229680Z",
     "start_time": "2019-12-09T19:21:58.226278Z"
    }
   },
   "source": [
    "## Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T00:01:31.457058Z",
     "start_time": "2019-12-10T00:01:31.441561Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).head(5)\n",
    "pd.DataFrame(y_train_log).head(5)\n",
    "pd.DataFrame(y_train_log_transf).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T00:01:34.964351Z",
     "start_time": "2019-12-10T00:01:34.954629Z"
    }
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).head(5)\n",
    "#pd.DataFrame(y_train_predict_log_transf).head(10)\n",
    "pd.DataFrame(scaler.inverse_transform(y_train_predict_log_transf)).head(10)\n",
    "#pd.DataFrame(np.exp(scaler.inverse_transform(y_train_predict_log_transf))).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T00:01:38.395699Z",
     "start_time": "2019-12-10T00:01:38.283072Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_test_transf = scaler.fit_transform(X_test)\n",
    "y_test_predict_log_transf = model.predict(X_test_transf)\n",
    "pd.DataFrame(y_test_predict_log_transf).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T00:01:41.498947Z",
     "start_time": "2019-12-10T00:01:41.481595Z"
    }
   },
   "outputs": [],
   "source": [
    "#y_test_transf_xxx = scaler.fit_transform(y_train)\n",
    "scaler = StandardScaler()\n",
    "y_train_log_transf_xxx = scaler.fit_transform(np.log(y_train))\n",
    "y_test_predic_log  = scaler.inverse_transform(y_test_predict_log_transf)\n",
    "pd.DataFrame(y_test_predic_log).head(10)\n",
    "y_test_predic  = np.exp(y_test_predic_log)\n",
    "pd.DataFrame(y_test_predic).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T00:01:44.138463Z",
     "start_time": "2019-12-10T00:01:44.121836Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_id = pd.DataFrame(X_test_id)\n",
    "y_test_predic = pd.DataFrame(y_test_predic)\n",
    "df_sub = X_test_id.join(y_test_predic)\n",
    "print(df_sub.shape)\n",
    "col_names_subm = df_sub.columns.values\n",
    "col_names_subm[1] = 'SalePrice'\n",
    "df_sub.columns = col_names_subm\n",
    "\n",
    "df_sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T00:01:48.383099Z",
     "start_time": "2019-12-10T00:01:48.353927Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv('hp_all_nn2-opti.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T00:01:56.426749Z",
     "start_time": "2019-12-10T00:01:50.569707Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import kaggle\n",
    "submission_file = \"hp_all_nn2-opti.csv\"\n",
    "kaggle.api.competition_submit(submission_file, \"all_nn2-opti\", \"house-prices-advanced-regression-techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
